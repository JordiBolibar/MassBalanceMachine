{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, train_test_split, GroupShuffleSplit\n",
    "\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "from oggm import cfg, utils, workflow, tasks\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "import config\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(config.SEED)\n",
    "\n",
    "# in case no memory\n",
    "#Â free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "color_palette_glaciers = sns.color_palette(get_cmap_hex(cmap, 15))\n",
    "\n",
    "# For bars and lines:\n",
    "# color_diff_xgb = '#878787'\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 2)\n",
    "color_xgb = colors[0]\n",
    "color_tim = '#c51b7d'\n",
    "\n",
    "# Violin and boxplots:\n",
    "colors_temp_freq = sns.color_palette(get_cmap_hex(cm.devon, 8))\n",
    "boxplot_style = {\n",
    "    \"width\": .6,\n",
    "    \"showcaps\": False,\n",
    "    \"palette\": colors_temp_freq,\n",
    "    \"flierprops\": {\n",
    "        \"marker\": \"x\"\n",
    "    },\n",
    "    \"showmeans\": True,\n",
    "    \"meanprops\": {\n",
    "        \"markerfacecolor\": \"white\"\n",
    "    }\n",
    "}\n",
    "\n",
    "marker_tim = 's'\n",
    "marker_xgb = 'o'\n",
    "marker_std = '_'\n",
    "\n",
    "custom_working_dir = '../../../data/OGGM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "path_rgi = '../../../data/GLAMOS/CH_glacier_ids_long.csv'\n",
    "rgi_df = pd.read_csv(path_rgi, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(path_PMB_GLAMOS_csv + 'CH_wgms_dataset.csv')\n",
    "data_glamos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gl = data_glamos.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', figsize=(15, 5), cmap=cmap)\n",
    "plt.title('Number of total measurements per glacier since 1961')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_glamos.GLACIER.unique()), data_glamos.GLACIER.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'gries'\n",
    "rgi_gl = rgi_df.loc[glacierName]['rgi_id.v6']\n",
    "data_gl = data_glamos[data_glamos.RGIId == rgi_gl]\n",
    "dataset_gl = mbm.Dataset(data=data_gl,\n",
    "                         region_name='CH',\n",
    "                         data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "ds, glacier_indices, gdir = dataset_gl.get_glacier_mask(custom_working_dir)\n",
    "\n",
    "dataset_gl.data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot glacier attributes\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "ds.masked_slope.plot(ax=ax[0], cmap=cm.batlow)\n",
    "ax[0].set_title('Slope')\n",
    "ds.masked_elev.plot(ax=ax[1], cmap=cm.batlow)\n",
    "ax[1].set_title('Elevation')\n",
    "ds.masked_aspect.plot(ax=ax[2], cmap=cm.batlow)\n",
    "ax[2].set_title('Aspect')\n",
    "ds.masked_dis.plot(ax=ax[3], cmap=cm.batlow)\n",
    "ax[3].set_title('Dis from border')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe of glacier grid\n",
    "years = data_gl['YEAR'].unique()\n",
    "print('Number of years: {} from {} to {}'.format(len(years), years[0],\n",
    "                                                 years[-1]))\n",
    "df_grid = dataset_gl.create_glacier_grid(custom_working_dir)\n",
    "# Add metadata that is not in WGMS dataset\n",
    "df_grid[\"PERIOD\"] = \"annual\"\n",
    "df_grid['GLACIER'] = glacierName\n",
    "print('Length of df_grid:', len(df_grid))\n",
    "df_grid.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coordinates\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "df_grid_one_year = df_grid[df_grid.YEAR == 2006]\n",
    "ax.scatter(df_grid_one_year.POINT_LON,\n",
    "           df_grid_one_year.POINT_LAT,\n",
    "           s=1,\n",
    "           label='OGGM grid',\n",
    "           marker='o',\n",
    "           color=color_xgb)\n",
    "ax.scatter(data_gl.POINT_LON,\n",
    "           data_gl.POINT_LAT,\n",
    "           s=8,\n",
    "           label='stakes',\n",
    "           marker='x',\n",
    "           color=color_tim)\n",
    "ax.legend()\n",
    "ax.set_title(f'OGGM grid and GLAMOS stakes for {df_grid.GLACIER.iloc[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add climate variables & convert to monthly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "voi_topographical = ['aspect', 'slope', 'dis_from_border', 'topo']\n",
    "# voi_topographical = ['aspect', 'slope']\n",
    "\n",
    "RUN = False\n",
    "if RUN == True:\n",
    "    dataset_grid = mbm.Dataset(data=df_grid,\n",
    "                               region_name='CH',\n",
    "                               data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "    # Add climate data:\n",
    "    # Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "    era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "    geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "    # Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "    dataset_grid.get_climate_features(climate_data=era5_climate_data,\n",
    "                                      geopotential_data=geopotential_data,\n",
    "                                      change_units=True)\n",
    "    print('Shape after adding climate variables:', dataset_grid.data.shape)\n",
    "\n",
    "    # For each record, convert to a monthly time resolution\n",
    "    dataset_grid.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                                    vois_climate=vois_climate,\n",
    "                                    vois_topographical=voi_topographical)\n",
    "    print('Shape after converting to monthly format:', dataset_grid.data.shape)\n",
    "\n",
    "    # Save grid:\n",
    "    dataset_grid.data.to_csv(path_glacier_grid + f'{glacierName}_grid.csv',\n",
    "                             index=False)\n",
    "\n",
    "df_grid = pd.read_csv(path_glacier_grid + f'{glacierName}_grid.csv')\n",
    "\n",
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader = mbm.DataLoader(data=df_grid,\n",
    "                            random_seed=config.SEED,\n",
    "                            meta_data_columns=config.META_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gl = data_glamos[data_glamos.RGIId == rgi_gl]\n",
    "print('Number of winter and annual samples:', len(data_gl))\n",
    "print('Number of annual samples:', len(data_gl[data_gl.PERIOD == 'annual']))\n",
    "print('Number of winter samples:', len(data_gl[data_gl.PERIOD == 'winter']))\n",
    "\n",
    "# change mm w.e. to m w.e.\n",
    "data_gl['POINT_BALANCE'] = data_gl['POINT_BALANCE'] / 1000\n",
    "\n",
    "dataset_gl = mbm.Dataset(data=data_gl,\n",
    "                         region_name='CH',\n",
    "                         data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "# Plot number of measurements per year\n",
    "# Number of measurements per glacier per year:\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "num_gl_yr = data_gl.groupby(['YEAR', 'PERIOD']).size().unstack().reset_index()\n",
    "num_gl_yr.plot(x='YEAR',\n",
    "               kind='bar',\n",
    "               stacked=True,\n",
    "               ax=ax,\n",
    "               title=f'{glacierName}',\n",
    "               colormap=cm.glasgow)\n",
    "ax.set_ylabel('Number of measurements')\n",
    "ax.set_title(f'Number of measurements per year: {glacierName}', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add climate data:\n",
    "# Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "# Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "dataset_gl.get_climate_features(climate_data=era5_climate_data,\n",
    "                                geopotential_data=geopotential_data,\n",
    "                                change_units=True)\n",
    "\n",
    "# For each record, convert to a monthly time resolution\n",
    "dataset_gl.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                              vois_climate=vois_climate,\n",
    "                              vois_topographical=voi_topographical)\n",
    "\n",
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader_gl = mbm.DataLoader(data=dataset_gl.data,\n",
    "                               random_seed=config.SEED,\n",
    "                               meta_data_columns=config.META_DATA)\n",
    "TYPE_SPLIT = 'year'\n",
    "if TYPE_SPLIT == 'year':\n",
    "    # Split into training and test years with train_test_split\n",
    "    train_years, test_years = train_test_split(dataset_gl.data.YEAR.unique(),\n",
    "                                               test_size=0.2,\n",
    "                                               random_state=config.SEED)\n",
    "\n",
    "    train_indices = dataset_gl.data[dataset_gl.data.YEAR.isin(\n",
    "        train_years)].index\n",
    "    test_indices = dataset_gl.data[dataset_gl.data.YEAR.isin(test_years)].index\n",
    "\n",
    "    dataloader_gl.set_custom_train_test_indices(train_indices, test_indices)\n",
    "\n",
    "else:\n",
    "    # Randomly (though does not separate meas ID)\n",
    "    train_itr, test_itr = dataloader_gl.set_train_test_split(test_size=0.2,\n",
    "                                                             shuffle=True)\n",
    "\n",
    "    train_indices, test_indices = list(train_itr), list(test_itr)\n",
    "\n",
    "# Get the features and targets of the training data for the indices as defined above, that will be used during the cross validation.\n",
    "df_X_train = dataset_gl.data.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get test set\n",
    "df_X_test = dataset_gl.data.iloc[test_indices]\n",
    "y_test = df_X_test['POINT_BALANCE'].values\n",
    "\n",
    "# Create the CV splits based on the training dataset. The default value for the number of splits is 5.\n",
    "splits = dataloader_gl.get_cv_split(n_splits=5, type_fold='group-meas-id')\n",
    "test_meas_id = df_X_test['ID'].unique()\n",
    "\n",
    "# Years in training and test set\n",
    "train_years = df_X_train.YEAR.unique()\n",
    "test_years = df_X_test.YEAR.unique()\n",
    "print('Train years:', train_years)\n",
    "print('Test years:', test_years)\n",
    "\n",
    "# Plot splits\n",
    "visualiseSplits(y_test, y_train, splits, colors=[color_xgb, color_tim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "parameters = {\n",
    "    'max_depth': [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "custom_xgboost.randomsearch(\n",
    "    parameters=parameters,\n",
    "    n_iter=20,\n",
    "    splits=splits,\n",
    "    features=df_X_train,\n",
    "    targets=y_train,\n",
    "    num_jobs=-1,\n",
    "    random_seed=config.SEED,\n",
    ")\n",
    "\n",
    "# save best model\n",
    "custom_xgboost.save_model(f'xgb_{glacierName}.pkl')\n",
    "\n",
    "best_params = params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    df_X_test, config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(df_X_test, y_test)  # negative\n",
    "mse, rmse, mae, pearson_corr = xgb.evalMetrics(metadata_test, y_pred, y_test)\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = df_X_test.copy()\n",
    "df_pred['target'] = y_test\n",
    "grouped_ids = df_pred.groupby('ID').agg({'target': 'mean'})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = df_X_test.groupby('ID')['PERIOD'].first()\n",
    "\n",
    "predVSTruth(grouped_ids,\n",
    "            mae,\n",
    "            rmse,\n",
    "            pearson_corr,\n",
    "            title=f'XGBoost on {glacierName} (split years)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['ELEVATION_DIFFERENCE'] + voi_topographical + vois_climate\n",
    "FIPlot(best_estimator, feature_columns, vois_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions for whole grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best estimator from grid search:\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_init = {}\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "feature_columns = [\n",
    "    'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'aspect', 'fal', 'slhf',\n",
    "    'slope', 'sshf', 'ssrd', 'str', 't2m', 'tp'\n",
    "]\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(\n",
    "    meta_data_columns=config.META_DATA, **param_init)\n",
    "clf = custom_xgboost.load_model(f'xgb_{glacierName}.pkl')\n",
    "print('Params:', clf.best_params_)\n",
    "xgb = clf.best_estimator_\n",
    "\n",
    "# Make predictions:\n",
    "print('Shape of test data:', df_grid.shape)\n",
    "\n",
    "# Set to CPU for predictions:\n",
    "xgb = xgb.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on whole glacier grid\n",
    "features_grid, metadata_grid = xgb._create_features_metadata(\n",
    "    df_grid, config.META_DATA)\n",
    "y_pred_grid = xgb.predict(features_grid)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_grid_agg = xgb.aggrPredict(metadata_grid, config.META_DATA,\n",
    "                                  features_grid)\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "grouped_ids = df_grid.groupby('ID').agg({'YEAR': 'mean'})\n",
    "grouped_ids['pred'] = y_pred_grid_agg\n",
    "\n",
    "# Sum over all points of a glacier to get glacier wide SMB\n",
    "grouped_ids = grouped_ids.groupby('YEAR').mean()\n",
    "\n",
    "df_target = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                        f'{glacierName}_fix.csv')\n",
    "df_target = transformDates(df_target)\n",
    "# Remove obvious duplicates:\n",
    "df_target = df_target.drop_duplicates()\n",
    "df_target['YEAR'] = df_target['date1'].apply(lambda x: pd.to_datetime(x).year)\n",
    "df_target['Annual Balance'] = df_target['Annual Balance'] / (1000)\n",
    "df_target = df_target[['YEAR', 'Annual Balance']].set_index('YEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "grouped_ids.plot(y='pred', label='Predicted SMB', ax=ax, color = color_xgb)\n",
    "df_target.plot(y='Annual Balance', label='GLAMOS SMB', ax=ax, color = color_tim)\n",
    "\n",
    "ax.set_title(f'{glacierName} SMB')\n",
    "ax.set_ylabel('SMB (m w.e.)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot distribution of input variables:\n",
    "# fig = plt.figure(figsize=(15, 10))\n",
    "# for i, feature in enumerate(feature_columns):\n",
    "#     ax = plt.subplot(3, 5, i + 1)\n",
    "#     sns.histplot(data=df_grid[feature],\n",
    "#                  color='blue',\n",
    "#                  alpha=0.5,\n",
    "#                  kde=True,\n",
    "#                  label='Grid',\n",
    "#                  ax=ax,\n",
    "#                  stat='density')\n",
    "#     sns.histplot(data=df_X_test[feature],\n",
    "#                  color='orange',\n",
    "#                  alpha=0.5,\n",
    "#                  kde=True,\n",
    "#                  label='Stakes',\n",
    "#                  ax=ax,\n",
    "#                  stat='density')\n",
    "#     ax.legend()\n",
    "#     ax.set_title(feature)\n",
    "#     ax.set_xlabel('')\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silvretta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'silvretta'\n",
    "rgi_gl = rgi_df.loc[glacierName]['rgi_id.v6']\n",
    "data_gl = data_glamos[data_glamos.RGIId == rgi_gl]\n",
    "dataset_gl = mbm.Dataset(data=data_gl,\n",
    "                         region_name='CH',\n",
    "                         data_path=path_PMB_GLAMOS_csv)\n",
    "dataset_gl.data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_working_dir = '../../../data/OGGM/'\n",
    "ds, glacier_indices, gdir = dataset_gl.get_glacier_mask(custom_working_dir)\n",
    "\n",
    "# Plot glacier attributes\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "ds.masked_slope.plot(ax=ax[0])\n",
    "ax[0].set_title('Slope')\n",
    "ds.masked_elev.plot(ax=ax[1])\n",
    "ax[1].set_title('Elevation')\n",
    "ds.masked_aspect.plot(ax=ax[2])\n",
    "ax[2].set_title('Aspect')\n",
    "ds.masked_dis.plot(ax=ax[3])\n",
    "ax[3].set_title('Dis from border')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe of glacier grid\n",
    "years = data_gl['YEAR'].unique()\n",
    "print('Number of years: {} from {} to {}'.format(len(years), years[0],\n",
    "                                                 years[-1]))\n",
    "df_grid = dataset_gl.create_glacier_grid(custom_working_dir)\n",
    "df_grid[\"PERIOD\"] = \"annual\"\n",
    "df_grid['GLACIER'] = glacierName\n",
    "print('Length of df_grid:', len(df_grid))\n",
    "df_grid.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coordinates\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "df_grid_one_year = df_grid[df_grid.YEAR == 1984]\n",
    "ax.scatter(df_grid_one_year.POINT_LON,\n",
    "           df_grid_one_year.POINT_LAT,\n",
    "           s=1,\n",
    "           label='OGGM grid')\n",
    "ax.scatter(data_gl.POINT_LON,\n",
    "           data_gl.POINT_LAT,\n",
    "           s=8,\n",
    "           label='stakes',\n",
    "           marker='x')\n",
    "ax.legend()\n",
    "ax.set_title(f'OGGM grid and GLAMOS stakes for {df_grid.GLACIER.iloc[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add climate variables & convert to monthly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "# voi_topographical = ['aspect', 'slope', 'dis_from_border', 'topo']\n",
    "voi_topographical = ['aspect', 'slope']\n",
    "# meta_data_columns = [\"RGIId\", \"POINT_ID\", \"ID\", \"N_MONTHS\", \"MONTHS\", \"PERIOD\"]\n",
    "\n",
    "RUN = False\n",
    "if RUN == True:\n",
    "    # Provide the column name for the column that has the RGI IDs for each of the stakes\n",
    "    dataset_grid = mbm.Dataset(data=df_grid,\n",
    "                               region_name='CH',\n",
    "                               data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "    # Add climate data:\n",
    "    # Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "    era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "    geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "    # Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "    dataset_grid.get_climate_features(climate_data=era5_climate_data,\n",
    "                                      geopotential_data=geopotential_data,\n",
    "                                      change_units=True)\n",
    "    print('Shape after adding climate variables:', dataset_grid.data.shape)\n",
    "\n",
    "    # For each record, convert to a monthly time resolution\n",
    "    dataset_grid.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                                    vois_climate=vois_climate,\n",
    "                                    vois_topographical=voi_topographical)\n",
    "    print('Shape after converting to monthly format:', dataset_grid.data.shape)\n",
    "\n",
    "    # Save grid:\n",
    "    dataset_grid.data.to_csv(path_glacier_grid + f'{glacierName}_grid.csv',\n",
    "                             index=False)\n",
    "\n",
    "df_grid = pd.read_csv(path_glacier_grid + f'{glacierName}_grid.csv')\n",
    "\n",
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader = mbm.DataLoader(data=df_grid,\n",
    "                            random_seed=config.SEED,\n",
    "                            meta_data_columns=config.META_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gl = data_glamos[data_glamos.RGIId == rgi_gl]\n",
    "print('Number of winter and annual samples:', len(data_gl))\n",
    "print('Number of annual samples:', len(data_gl[data_gl.PERIOD == 'annual']))\n",
    "print('Number of winter samples:', len(data_gl[data_gl.PERIOD == 'winter']))\n",
    "\n",
    "# change mm w.e. to m w.e.\n",
    "data_gl['POINT_BALANCE'] = data_gl['POINT_BALANCE'] / 1000\n",
    "\n",
    "dataset_gl = mbm.Dataset(data=data_gl,\n",
    "                         region_name='CH',\n",
    "                         data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "# Plot number of measurements per year\n",
    "# Number of measurements per glacier per year:\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "num_gl_yr = data_gl.groupby(['YEAR', 'PERIOD']).size().unstack().reset_index()\n",
    "num_gl_yr.plot(x='YEAR', kind='bar', stacked=True, ax=ax, title='Gries')\n",
    "ax.set_ylabel('Number of measurements')\n",
    "ax.set_title(f'Number of measurements per year: {glacierName}', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add climate data:\n",
    "# Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "# Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "dataset_gl.get_climate_features(climate_data=era5_climate_data,\n",
    "                                geopotential_data=geopotential_data,\n",
    "                                change_units=True)\n",
    "\n",
    "# For each record, convert to a monthly time resolution\n",
    "dataset_gl.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                              vois_climate=vois_climate,\n",
    "                              vois_topographical=voi_topographical)\n",
    "\n",
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader_gl = mbm.DataLoader(data=dataset_gl.data,\n",
    "                               random_seed=config.SEED,\n",
    "                               meta_data_columns=config.META_DATA)\n",
    "TYPE_SPLIT = 'year'\n",
    "if TYPE_SPLIT == 'year':\n",
    "    # Split into training and test years with train_test_split\n",
    "    train_years, test_years = train_test_split(dataset_gl.data.YEAR.unique(),\n",
    "                                               test_size=0.2,\n",
    "                                               random_state=config.SEED)\n",
    "\n",
    "    train_indices = dataset_gl.data[dataset_gl.data.YEAR.isin(\n",
    "        train_years)].index\n",
    "    test_indices = dataset_gl.data[dataset_gl.data.YEAR.isin(test_years)].index\n",
    "\n",
    "    dataloader_gl.set_custom_train_test_indices(train_indices, test_indices)\n",
    "\n",
    "else:\n",
    "    # Randomly (though does not separate meas ID)\n",
    "    train_itr, test_itr = dataloader_gl.set_train_test_split(test_size=0.2,\n",
    "                                                             shuffle=True)\n",
    "\n",
    "    train_indices, test_indices = list(train_itr), list(test_itr)\n",
    "\n",
    "# Get the features and targets of the training data for the indices as defined above, that will be used during the cross validation.\n",
    "df_X_train = dataset_gl.data.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get test set\n",
    "df_X_test = dataset_gl.data.iloc[test_indices]\n",
    "y_test = df_X_test['POINT_BALANCE'].values\n",
    "\n",
    "# Create the CV splits based on the training dataset. The default value for the number of splits is 5.\n",
    "splits = dataloader_gl.get_cv_split(n_splits=5, type_fold='group-meas-id')\n",
    "test_meas_id = df_X_test['ID'].unique()\n",
    "\n",
    "# Years in training and test set\n",
    "train_years = df_X_train.YEAR.unique()\n",
    "test_years = df_X_test.YEAR.unique()\n",
    "print('Train years:', train_years)\n",
    "print('Test years:', test_years)\n",
    "\n",
    "# Plot splits\n",
    "visualiseSplits(y_test, y_train, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "parameters = {\n",
    "    'max_depth': [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "feature_columns = [\n",
    "    'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'aspect', 'fal', 'slhf',\n",
    "    'slope', 'sshf', 'ssrd', 'str', 't2m', 'tp'\n",
    "]\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "custom_xgboost.randomsearch(\n",
    "    parameters=parameters,\n",
    "    n_iter=20,\n",
    "    splits=splits,\n",
    "    features=df_X_train,\n",
    "    targets=y_train,\n",
    "    num_jobs=-1,\n",
    "    random_seed=config.SEED,\n",
    ")\n",
    "\n",
    "# save best model\n",
    "custom_xgboost.save_model(f'xgb_{glacierName}.pkl')\n",
    "\n",
    "best_params = params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    df_X_test, config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(df_X_test, y_test)  # negative\n",
    "mse, rmse, mae, pearson_corr = xgb.evalMetrics(metadata_test, y_pred, y_test)\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = df_X_test.copy()\n",
    "df_pred['target'] = y_test\n",
    "grouped_ids = df_pred.groupby('ID').agg({'target': 'mean'})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = df_X_test.groupby('ID')['PERIOD'].first()\n",
    "\n",
    "predVSTruth(grouped_ids,\n",
    "            mae,\n",
    "            rmse,\n",
    "            pearson_corr,\n",
    "            title=f'XGBoost on {glacierName} (split years)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions for whole grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best estimator from grid search:\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_init = {}\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "feature_columns = [\n",
    "    'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'aspect', 'fal', 'slhf',\n",
    "    'slope', 'sshf', 'ssrd', 'str', 't2m', 'tp'\n",
    "]\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "clf = custom_xgboost.load_model(f'xgb_{glacierName}.pkl')\n",
    "print('Params:', clf.best_params_)\n",
    "xgb = clf.best_estimator_\n",
    "\n",
    "# Make predictions:\n",
    "print('Shape of test data:', df_grid.shape)\n",
    "\n",
    "# Set to CPU for predictions:\n",
    "xgb = xgb.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on whole glacier grid\n",
    "features_grid, metadata_grid = xgb._create_features_metadata(\n",
    "    df_grid, config.META_DATA)\n",
    "y_pred_grid = xgb.predict(features_grid)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_grid_agg = xgb.aggrPredict(metadata_grid, config.META_DATA,\n",
    "                                  features_grid)\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "grouped_ids = df_grid.groupby('ID').agg({'YEAR': 'mean'})\n",
    "grouped_ids['pred'] = y_pred_grid_agg\n",
    "\n",
    "# Sum over all points of a glacier to get glacier wide SMB\n",
    "grouped_ids = grouped_ids.groupby('YEAR').mean()\n",
    "\n",
    "df_target = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                        f'{glacierName}_fix.csv')\n",
    "df_target = transformDates(df_target)\n",
    "# Remove obvious duplicates:\n",
    "df_target = df_target.drop_duplicates()\n",
    "df_target['YEAR'] = df_target['date1'].apply(lambda x: pd.to_datetime(x).year)\n",
    "df_target = df_target[df_target['YEAR'] > 1960]\n",
    "df_target['Annual Balance'] = df_target['Annual Balance'] / (1000)\n",
    "df_target = df_target[['YEAR', 'Annual Balance']].set_index('YEAR')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "grouped_ids.plot(y='pred', label='Predicted SMB', ax=ax)\n",
    "df_target.plot(y='Annual Balance', label='GLAMOS SMB', ax=ax)\n",
    "\n",
    "ax.set_title(f'{glacierName} SMB')\n",
    "ax.set_ylabel('SMB (m w.e.)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aletsch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'aletsch'\n",
    "rgi_gl = rgi_df.loc[glacierName]['rgi_id.v6']\n",
    "data_gl = data_glamos[data_glamos.RGIId == rgi_gl]\n",
    "dataset_gl = mbm.Dataset(data=data_gl,\n",
    "                         region_name='CH',\n",
    "                         data_path=path_PMB_GLAMOS_csv)\n",
    "dataset_gl.data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_working_dir = '../../../data/OGGM/'\n",
    "ds, glacier_indices, gdir = dataset_gl.get_glacier_mask(custom_working_dir)\n",
    "\n",
    "# Plot glacier attributes\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "ds.masked_slope.plot(ax=ax[0])\n",
    "ax[0].set_title('Slope')\n",
    "ds.masked_elev.plot(ax=ax[1])\n",
    "ax[1].set_title('Elevation')\n",
    "ds.masked_aspect.plot(ax=ax[2])\n",
    "ax[2].set_title('Aspect')\n",
    "ds.masked_dis.plot(ax=ax[3])\n",
    "ax[3].set_title('Dis from border')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe of glacier grid\n",
    "years = data_gl['YEAR'].unique()\n",
    "print('Number of years: {} from {} to {}'.format(len(years), years[0],\n",
    "                                                 years[-1]))\n",
    "df_grid = dataset_gl.create_glacier_grid(custom_working_dir)\n",
    "df_grid[\"PERIOD\"] = \"annual\"\n",
    "df_grid['GLACIER'] = glacierName\n",
    "print('Length of df_grid:', len(df_grid))\n",
    "\n",
    "# Plot coordinates\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "df_grid_one_year = df_grid[df_grid.YEAR == 1984]\n",
    "ax.scatter(df_grid_one_year.POINT_LON,\n",
    "           df_grid_one_year.POINT_LAT,\n",
    "           s=1,\n",
    "           label='OGGM grid')\n",
    "ax.scatter(data_gl.POINT_LON,\n",
    "           data_gl.POINT_LAT,\n",
    "           s=8,\n",
    "           label='stakes',\n",
    "           marker='x')\n",
    "ax.legend()\n",
    "ax.set_title(f'OGGM grid and GLAMOS stakes for {df_grid.GLACIER.iloc[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the short names of the climate variables available in the dataset\n",
    "vois_climate = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str']\n",
    "# voi_topographical = ['aspect', 'slope', 'dis_from_border', 'topo']\n",
    "voi_topographical = ['aspect', 'slope']\n",
    "# meta_data_columns = [\"RGIId\", \"POINT_ID\", \"ID\", \"N_MONTHS\", \"MONTHS\", \"PERIOD\"]\n",
    "\n",
    "RUN = False\n",
    "if RUN == True:\n",
    "    # Provide the column name for the column that has the RGI IDs for each of the stakes\n",
    "    dataset_grid = mbm.Dataset(data=df_grid,\n",
    "                               region_name='CH',\n",
    "                               data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "    # Add climate data:\n",
    "    # Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "    era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "    geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "    # Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "    dataset_grid.get_climate_features(climate_data=era5_climate_data,\n",
    "                                      geopotential_data=geopotential_data,\n",
    "                                      change_units=True)\n",
    "    print('Shape after adding climate variables:', dataset_grid.data.shape)\n",
    "\n",
    "    # For each record, convert to a monthly time resolution\n",
    "    dataset_grid.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                                    vois_climate=vois_climate,\n",
    "                                    vois_topographical=voi_topographical)\n",
    "    print('Shape after converting to monthly format:', dataset_grid.data.shape)\n",
    "\n",
    "    # Save grid:\n",
    "    dataset_grid.data.to_csv(path_glacier_grid + f'{glacierName}_grid.csv',\n",
    "                             index=False)\n",
    "\n",
    "df_grid = pd.read_csv(path_glacier_grid + f'{glacierName}_grid.csv')\n",
    "\n",
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader = mbm.DataLoader(data=df_grid,\n",
    "                            random_seed=config.SEED,\n",
    "                            meta_data_columns=config.META_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gl = data_glamos[data_glamos.RGIId == rgi_gl]\n",
    "print('Number of winter and annual samples:', len(data_gl))\n",
    "print('Number of annual samples:', len(data_gl[data_gl.PERIOD == 'annual']))\n",
    "print('Number of winter samples:', len(data_gl[data_gl.PERIOD == 'winter']))\n",
    "\n",
    "# change mm w.e. to m w.e.\n",
    "data_gl['POINT_BALANCE'] = data_gl['POINT_BALANCE'] / 1000\n",
    "\n",
    "dataset_gl = mbm.Dataset(data=data_gl,\n",
    "                         region_name='CH',\n",
    "                         data_path=path_PMB_GLAMOS_csv)\n",
    "\n",
    "# Plot number of measurements per year\n",
    "# Number of measurements per glacier per year:\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "num_gl_yr = data_gl.groupby(['YEAR', 'PERIOD']).size().unstack().reset_index()\n",
    "num_gl_yr.plot(x='YEAR', kind='bar', stacked=True, ax=ax, title='Gries')\n",
    "ax.set_ylabel('Number of measurements')\n",
    "ax.set_title(f'Number of measurements per year: {glacierName}', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add climate data:\n",
    "# Specify the files of the climate data, that will be matched with the coordinates of the stake data\n",
    "era5_climate_data = path_ERA5_raw + 'era5_monthly_averaged_data.nc'\n",
    "geopotential_data = path_ERA5_raw + 'era5_geopotential_pressure.nc'\n",
    "\n",
    "# Match the climate features, from the ERA5Land netCDF file, for each of the stake measurement dataset\n",
    "dataset_gl.get_climate_features(climate_data=era5_climate_data,\n",
    "                                geopotential_data=geopotential_data,\n",
    "                                change_units=True)\n",
    "\n",
    "# For each record, convert to a monthly time resolution\n",
    "dataset_gl.convert_to_monthly(meta_data_columns=config.META_DATA,\n",
    "                              vois_climate=vois_climate,\n",
    "                              vois_topographical=voi_topographical)\n",
    "\n",
    "# Create a new DataLoader object with the monthly stake data measurements.\n",
    "dataloader_gl = mbm.DataLoader(data=dataset_gl.data,\n",
    "                               random_seed=config.SEED,\n",
    "                               meta_data_columns=config.META_DATA)\n",
    "TYPE_SPLIT = 'year'\n",
    "if TYPE_SPLIT == 'year':\n",
    "    # Split into training and test years with train_test_split\n",
    "    train_years, test_years = train_test_split(dataset_gl.data.YEAR.unique(),\n",
    "                                               test_size=0.2,\n",
    "                                               random_state=config.SEED)\n",
    "\n",
    "    train_indices = dataset_gl.data[dataset_gl.data.YEAR.isin(\n",
    "        train_years)].index\n",
    "    test_indices = dataset_gl.data[dataset_gl.data.YEAR.isin(test_years)].index\n",
    "\n",
    "    dataloader_gl.set_custom_train_test_indices(train_indices, test_indices)\n",
    "\n",
    "else:\n",
    "    # Randomly (though does not separate meas ID)\n",
    "    train_itr, test_itr = dataloader_gl.set_train_test_split(test_size=0.2,\n",
    "                                                             shuffle=True)\n",
    "\n",
    "    train_indices, test_indices = list(train_itr), list(test_itr)\n",
    "\n",
    "# Get the features and targets of the training data for the indices as defined above, that will be used during the cross validation.\n",
    "df_X_train = dataset_gl.data.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get test set\n",
    "df_X_test = dataset_gl.data.iloc[test_indices]\n",
    "y_test = df_X_test['POINT_BALANCE'].values\n",
    "\n",
    "# Create the CV splits based on the training dataset. The default value for the number of splits is 5.\n",
    "splits = dataloader_gl.get_cv_split(n_splits=5, type_fold='group-meas-id')\n",
    "test_meas_id = df_X_test['ID'].unique()\n",
    "\n",
    "# Years in training and test set\n",
    "train_years = df_X_train.YEAR.unique()\n",
    "test_years = df_X_test.YEAR.unique()\n",
    "print('Train years:', train_years)\n",
    "print('Test years:', test_years)\n",
    "\n",
    "# Plot splits\n",
    "visualiseSplits(y_test, y_train, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "parameters = {\n",
    "    'max_depth': [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "feature_columns = [\n",
    "    'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'aspect', 'fal', 'slhf',\n",
    "    'slope', 'sshf', 'ssrd', 'str', 't2m', 'tp'\n",
    "]\n",
    "\n",
    "param_init = {}\n",
    "param_init['device'] = 'cuda:0'\n",
    "param_init['tree_method'] = 'hist'\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "custom_xgboost.randomsearch(\n",
    "    parameters=parameters,\n",
    "    n_iter=20,\n",
    "    splits=splits,\n",
    "    features=df_X_train,\n",
    "    targets=y_train,\n",
    "    num_jobs=-1,\n",
    "    random_seed=config.SEED,\n",
    ")\n",
    "\n",
    "# save best model\n",
    "custom_xgboost.save_model(f'xgb_{glacierName}.pkl')\n",
    "\n",
    "best_params = params = custom_xgboost.param_search.best_params_\n",
    "best_estimator = custom_xgboost.param_search.best_estimator_\n",
    "print(\"Best parameters:\\n\", best_params)\n",
    "print(\"Best score:\\n\", custom_xgboost.param_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to CPU for predictions:\n",
    "xgb = best_estimator.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on test\n",
    "features_test, metadata_test = xgb._create_features_metadata(\n",
    "    df_X_test, config.META_DATA)\n",
    "y_pred = xgb.predict(features_test)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_agg = xgb.aggrPredict(metadata_test, config.META_DATA, features_test)\n",
    "\n",
    "# Calculate scores\n",
    "score = xgb.score(df_X_test, y_test)  # negative\n",
    "mse, rmse, mae, pearson_corr = xgb.evalMetrics(metadata_test, y_pred, y_test)\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "df_pred = df_X_test.copy()\n",
    "df_pred['target'] = y_test\n",
    "grouped_ids = df_pred.groupby('ID').agg({'target': 'mean'})\n",
    "grouped_ids['pred'] = y_pred_agg\n",
    "grouped_ids['PERIOD'] = df_X_test.groupby('ID')['PERIOD'].first()\n",
    "\n",
    "predVSTruth(grouped_ids,\n",
    "            mae,\n",
    "            rmse,\n",
    "            pearson_corr,\n",
    "            title=f'XGBoost on {glacierName} (split years)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best estimator from grid search:\n",
    "# For each of the XGBoost parameter, define the grid range\n",
    "param_init = {}\n",
    "param_init[\"random_state\"] = config.SEED\n",
    "\n",
    "feature_columns = [\n",
    "    'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'aspect', 'fal', 'slhf',\n",
    "    'slope', 'sshf', 'ssrd', 'str', 't2m', 'tp'\n",
    "]\n",
    "\n",
    "# Create a CustomXGBoostRegressor instance\n",
    "custom_xgboost = mbm.models.CustomXGBoostRegressor(**param_init)\n",
    "clf = custom_xgboost.load_model(f'xgb_{glacierName}.pkl')\n",
    "print('Params:', clf.best_params_)\n",
    "xgb = clf.best_estimator_\n",
    "\n",
    "# Make predictions:\n",
    "print('Shape of test data:', df_grid.shape)\n",
    "\n",
    "# Set to CPU for predictions:\n",
    "xgb = xgb.set_params(device='cpu')\n",
    "\n",
    "# Make predictions on whole glacier grid\n",
    "features_grid, metadata_grid = xgb._create_features_metadata(\n",
    "    df_grid, config.META_DATA)\n",
    "y_pred_grid = xgb.predict(features_grid)\n",
    "\n",
    "# Make predictions aggr to meas ID:\n",
    "y_pred_grid_agg = xgb.aggrPredict(metadata_grid, config.META_DATA,\n",
    "                                  features_grid)\n",
    "\n",
    "# Aggregate predictions to annual or winter:\n",
    "grouped_ids = df_grid.groupby('ID').agg({'YEAR': 'mean'})\n",
    "grouped_ids['pred'] = y_pred_grid_agg\n",
    "\n",
    "# Sum over all points of a glacier to get glacier wide SMB\n",
    "grouped_ids = grouped_ids.groupby('YEAR').mean()\n",
    "\n",
    "df_target = pd.read_csv(path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                        f'{glacierName}_fix.csv')\n",
    "df_target = transformDates(df_target)\n",
    "# Remove obvious duplicates:\n",
    "df_target = df_target.drop_duplicates()\n",
    "df_target['YEAR'] = df_target['date1'].apply(lambda x: pd.to_datetime(x).year)\n",
    "df_target = df_target[df_target['YEAR'] > 1960]\n",
    "df_target['Annual Balance'] = df_target['Annual Balance'] / (1000)\n",
    "df_target = df_target[['YEAR', 'Annual Balance']].set_index('YEAR')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "grouped_ids.plot(y='pred', label='Predicted SMB', ax=ax)\n",
    "df_target.plot(y='Annual Balance', label='GLAMOS SMB', ax=ax)\n",
    "\n",
    "ax.set_title(f'{glacierName} SMB')\n",
    "ax.set_ylabel('SMB (m w.e.)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geodetic mass balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_csv = pd.read_csv(\n",
    "    '../../../data/GLAMOS/glacier-wide/volumechange_2023_r2023/volumechange_2023_r2023_old.csv',\n",
    "    sep=';')\n",
    "geodetic_csv = geodetic_csv.iloc[2:]  # remove unit rows\n",
    "geodetic_csv['glacier id'] = geodetic_csv['glacier id'].apply(\n",
    "    lambda x: x.split('-')[0].upper().strip() + '/' + x.split('-')[1].strip())\n",
    "geodetic_csv.rename(columns={\n",
    "    'glacier id': 'sgi-id',\n",
    "    'start date of observation': 'FROM_DATE',\n",
    "    'end date of observation': 'TO_DATE',\n",
    "    'annual geodetic mass balance': 'Bgeod'\n",
    "},\n",
    "                    inplace=True)\n",
    "geodetic_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgi_id = rgi_df.loc['silvretta']['sgi-id'].strip()\n",
    "silvretta_geoMB = geodetic_csv[geodetic_csv['sgi-id'] == sgi_id]\n",
    "\n",
    "\n",
    "# assign hydr. year\n",
    "def assignHydrYear(date):\n",
    "    date = pd.to_datetime(date)\n",
    "    return date.year\n",
    "\n",
    "\n",
    "silvretta_geoMB['FROM_YEAR'] = pd.to_datetime(\n",
    "    silvretta_geoMB['FROM_DATE']).apply(assignHydrYear) + 1\n",
    "silvretta_geoMB['TO_YEAR'] = pd.to_datetime(\n",
    "    silvretta_geoMB['TO_DATE']).apply(assignHydrYear)\n",
    "silvretta_geoMB = silvretta_geoMB[silvretta_geoMB['FROM_YEAR'] > 1961]\n",
    "silvretta_geoMB['B-Period'] = silvretta_geoMB['FROM_YEAR'].astype(\n",
    "    str) + '-' + silvretta_geoMB['TO_YEAR'].astype(str)\n",
    "silvretta_geoMB['Bgeod'] = silvretta_geoMB['Bgeod'].astype(float)\n",
    "silvretta_geoMB['volume change'] = silvretta_geoMB['volume change'].astype(\n",
    "    float)\n",
    "silvretta_geoMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodPred_ML, geodPred_TIM = [], []\n",
    "for i, row in silvretta_geoMB.iterrows():\n",
    "    geodPred_ML.append(\n",
    "        grouped_ids.loc[row.FROM_YEAR:row.TO_YEAR].mean().values[0])\n",
    "    geodPred_TIM.append(\n",
    "        df_target.loc[row.FROM_YEAR:row.TO_YEAR].mean().values[0])\n",
    "\n",
    "geodPred_df = pd.DataFrame({\n",
    "    'Bgeod':\n",
    "    np.concatenate(\n",
    "        [silvretta_geoMB['Bgeod'].values, geodPred_ML, geodPred_TIM]),\n",
    "    'Type':\n",
    "    np.concatenate([\n",
    "        np.tile('Bgeod', len(silvretta_geoMB)),\n",
    "        np.tile('ML', len(silvretta_geoMB)),\n",
    "        np.tile('PDD', len(silvretta_geoMB))\n",
    "    ]),\n",
    "    'Period':\n",
    "    np.concatenate([\n",
    "        silvretta_geoMB['B-Period'], silvretta_geoMB['B-Period'],\n",
    "        silvretta_geoMB['B-Period']\n",
    "    ])\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.barplot(geodPred_df,\n",
    "            x='Period',\n",
    "            y='Bgeod',\n",
    "            hue='Type',\n",
    "            ax=ax,\n",
    "            orient='v',\n",
    "            alpha=0.5)\n",
    "plt.tight_layout()\n",
    "ax.set_title('Silvretta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgi_id = rgi_df.loc['gries']['sgi-id'].strip()\n",
    "gries_geoMB = geodetic_csv[geodetic_csv['sgi-id'] == sgi_id]\n",
    "\n",
    "\n",
    "# assign hydr. year\n",
    "def assignHydrYear(date):\n",
    "    date = pd.to_datetime(date)\n",
    "    return date.year\n",
    "\n",
    "\n",
    "gries_geoMB['FROM_YEAR'] = pd.to_datetime(\n",
    "    gries_geoMB['FROM_DATE']).apply(assignHydrYear)\n",
    "gries_geoMB['TO_YEAR'] = pd.to_datetime(\n",
    "    gries_geoMB['TO_DATE']).apply(assignHydrYear)\n",
    "gries_geoMB = gries_geoMB[gries_geoMB['FROM_YEAR'] > 1961]\n",
    "gries_geoMB['B-Period'] = gries_geoMB['FROM_YEAR'].astype(\n",
    "    str) + '-' + gries_geoMB['TO_YEAR'].astype(str)\n",
    "gries_geoMB['Bgeod'] = gries_geoMB['Bgeod'].astype(float)\n",
    "gries_geoMB['volume change'] = gries_geoMB['volume change'].astype(float)\n",
    "\n",
    "geodPred_ML, geodPred_TIM = [], []\n",
    "for i, row in gries_geoMB.iterrows():\n",
    "    geodPred_ML.append(\n",
    "        grouped_ids.loc[row.FROM_YEAR:row.TO_YEAR].mean().values[0])\n",
    "    geodPred_TIM.append(\n",
    "        df_target.loc[row.FROM_YEAR:row.TO_YEAR].mean().values[0])\n",
    "\n",
    "geodPred_df = pd.DataFrame({\n",
    "    'Bgeod':\n",
    "    np.concatenate([gries_geoMB['Bgeod'].values, geodPred_ML, geodPred_TIM]),\n",
    "    'Type':\n",
    "    np.concatenate([\n",
    "        np.tile('Bgeod', len(gries_geoMB)),\n",
    "        np.tile('ML', len(gries_geoMB)),\n",
    "        np.tile('PDD', len(gries_geoMB))\n",
    "    ]),\n",
    "    'Period':\n",
    "    np.concatenate([\n",
    "        gries_geoMB['B-Period'], gries_geoMB['B-Period'],\n",
    "        gries_geoMB['B-Period']\n",
    "    ])\n",
    "})\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.barplot(geodPred_df,\n",
    "            x='Period',\n",
    "            y='Bgeod',\n",
    "            hue='Type',\n",
    "            ax=ax,\n",
    "            orient='v',\n",
    "            alpha=0.5)\n",
    "plt.tight_layout()\n",
    "ax.set_title('Gries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgi_id = rgi_df.loc['aletsch']['sgi-id'].strip()\n",
    "gries_geoMB = geodetic_csv[geodetic_csv['sgi-id'] == sgi_id]\n",
    "\n",
    "\n",
    "# assign hydr. year\n",
    "def assignHydrYear(date):\n",
    "    date = pd.to_datetime(date)\n",
    "    return date.year\n",
    "\n",
    "\n",
    "gries_geoMB['FROM_YEAR'] = pd.to_datetime(\n",
    "    gries_geoMB['FROM_DATE']).apply(assignHydrYear)\n",
    "gries_geoMB['TO_YEAR'] = pd.to_datetime(\n",
    "    gries_geoMB['TO_DATE']).apply(assignHydrYear)\n",
    "gries_geoMB = gries_geoMB[gries_geoMB['FROM_YEAR'] > 1961]\n",
    "gries_geoMB['B-Period'] = gries_geoMB['FROM_YEAR'].astype(\n",
    "    str) + '-' + gries_geoMB['TO_YEAR'].astype(str)\n",
    "gries_geoMB['Bgeod'] = gries_geoMB['Bgeod'].astype(float)\n",
    "gries_geoMB['volume change'] = gries_geoMB['volume change'].astype(float)\n",
    "\n",
    "geodPred_ML, geodPred_TIM = [], []\n",
    "for i, row in gries_geoMB.iterrows():\n",
    "    geodPred_ML.append(\n",
    "        grouped_ids.loc[row.FROM_YEAR:row.TO_YEAR].mean().values[0])\n",
    "    geodPred_TIM.append(\n",
    "        df_target.loc[row.FROM_YEAR:row.TO_YEAR].mean().values[0])\n",
    "\n",
    "geodPred_df = pd.DataFrame({\n",
    "    'Bgeod':\n",
    "    np.concatenate([gries_geoMB['Bgeod'].values, geodPred_ML, geodPred_TIM]),\n",
    "    'Type':\n",
    "    np.concatenate([\n",
    "        np.tile('Bgeod', len(gries_geoMB)),\n",
    "        np.tile('ML', len(gries_geoMB)),\n",
    "        np.tile('PDD', len(gries_geoMB))\n",
    "    ]),\n",
    "    'Period':\n",
    "    np.concatenate([\n",
    "        gries_geoMB['B-Period'], gries_geoMB['B-Period'],\n",
    "        gries_geoMB['B-Period']\n",
    "    ])\n",
    "})\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.barplot(geodPred_df,\n",
    "            x='Period',\n",
    "            y='Bgeod',\n",
    "            hue='Type',\n",
    "            ax=ax,\n",
    "            orient='v',\n",
    "            alpha=0.5)\n",
    "plt.tight_layout()\n",
    "ax.set_title('Aletsch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

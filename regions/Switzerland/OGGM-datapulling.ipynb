{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3ef86c",
   "metadata": {},
   "source": [
    "# OGGM - data pulling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eee480",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0d9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import oggm\n",
    "from oggm import cfg, utils, workflow, tasks, graphics\n",
    "from oggm import entity_task, global_tasks\n",
    "from oggm.utils import compile_climate_input\n",
    "from oggm.core import gis\n",
    "from oggm.utils import DEM_SOURCES\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pyproj\n",
    "from pyproj import Transformer\n",
    "import salem\n",
    "from tqdm import tqdm \n",
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdc8254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 07:26:32: oggm.cfg: Reading default parameters from the OGGM `params.cfg` configuration file.\n",
      "2024-07-31 07:26:32: oggm.cfg: Multiprocessing switched OFF according to the parameter file.\n",
      "2024-07-31 07:26:32: oggm.cfg: Multiprocessing: using all available processors (N=32)\n",
      "2024-07-31 07:26:32: oggm.cfg: PARAMS['border'] changed from `80` to `10`.\n",
      "2024-07-31 07:26:32: oggm.cfg: Multiprocessing switched ON after user settings.\n",
      "2024-07-31 07:26:32: oggm.cfg: PARAMS['continue_on_error'] changed from `False` to `True`.\n"
     ]
    }
   ],
   "source": [
    "from oggm import cfg, utils, workflow, tasks, graphics\n",
    "from oggm import entity_task, global_tasks\n",
    "from oggm.utils import compile_climate_input\n",
    "from oggm.core import gis\n",
    "from oggm.utils import DEM_SOURCES\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "cfg.PARAMS['border'] = 10\n",
    "cfg.PARAMS['use_multiprocessing'] = True \n",
    "cfg.PARAMS['continue_on_error'] = True\n",
    "# Module logger\n",
    "log = logging.getLogger('.'.join(__name__.split('.')[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e1ba9",
   "metadata": {},
   "source": [
    "## Download OGGM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fe5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "cfg.PATHS['working_dir'] = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98f767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_datetime(ds):\n",
    "    datetimes = []\n",
    "    for dec_year in ds.time.data:\n",
    "        year = int(dec_year)\n",
    "        rem = dec_year - year\n",
    "\n",
    "        base = datetime(year, 1, 1)\n",
    "        calendar_time = base + timedelta(seconds=(base.replace(year=base.year + 1) - base).total_seconds() * rem)\n",
    "        datetimes.append(calendar_time)\n",
    "        \n",
    "    climate_ds['time'] = datetimes\n",
    "        \n",
    "    return climate_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e5c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@entity_task(log)\n",
    "def get_gridded_features(gdir):\n",
    "    \"\"\"Retrieves and processes gridded and climate data for a gdir in order to create the dataset\n",
    "    to be fed to machine learning models.\n",
    "    \"\"\"\n",
    "    # Retrieve gridded data\n",
    "    with xr.open_dataset(gdir.get_filepath(\"gridded_data\")) as ds:\n",
    "        gridded_ds = ds.load()\n",
    "    # Retrieve climate data\n",
    "    with xr.open_dataset(gdir.get_filepath(\"climate_historical\")) as ds:\n",
    "        climate_ds = ds.load()\n",
    "\n",
    "    #### Climate data ####\n",
    "    # First we start by processing the climate data\n",
    "    # Trim climate dataset to desired period of the Hugonnet et al. (2021) dataset\n",
    "    climate_ds = climate_ds.sel(time=slice(\"2000-01-01\", \"2019-12-01\"))\n",
    "\n",
    "    # Now we downscale the climate data to the specific glacier\n",
    "    # Temperature\n",
    "    temps_2D = np.empty(\n",
    "        (\n",
    "            climate_ds.temp.data.size,\n",
    "            gridded_ds.topo.data.shape[0],\n",
    "            gridded_ds.topo.data.shape[1],\n",
    "        )\n",
    "    )\n",
    "    i = 0\n",
    "    for temp in climate_ds.temp.data:\n",
    "        temps_2D[i, :, :] = np.tile(temp, gridded_ds.topo.data.shape) + 6.0 / 1000.0 * (\n",
    "            gridded_ds.topo.data - climate_ds.ref_hgt.data\n",
    "        )\n",
    "        i = i + 1\n",
    "\n",
    "    PDD_2D = np.sum(np.where(temps_2D > 0.0, temps_2D, 0.0), axis=0)\n",
    "\n",
    "    # Rain\n",
    "    rain_period_2D = np.empty(\n",
    "        (\n",
    "            climate_ds.prcp.data.size,\n",
    "            gridded_ds.topo.data.shape[0],\n",
    "            gridded_ds.topo.data.shape[1],\n",
    "        )\n",
    "    )\n",
    "    i = 0\n",
    "    for prcp in climate_ds.prcp.data:\n",
    "        rain_period_2D[i, :, :] = np.tile(prcp, gridded_ds.topo.data.shape)\n",
    "        i = i + 1\n",
    "\n",
    "    rain_2D = np.sum(rain_period_2D, axis=0)\n",
    "\n",
    "    # Snow\n",
    "    snow_2D = np.where(temps_2D < 0.0, rain_period_2D, 0.0)\n",
    "\n",
    "    # Now we create a dictionary with the full dataset, including the previous climate data\n",
    "    # and all the interesting gridded datasets\n",
    "    training_data = {\n",
    "        \"PDD_2D\": PDD_2D,\n",
    "        \"rain_2D\": rain_2D,\n",
    "        \"snow_2D\": snow_2D,\n",
    "        \"topo\": gridded_ds.topo.data,\n",
    "        \"aspect\": gridded_ds.aspect.data,\n",
    "        \"slope\": gridded_ds.slope.data,\n",
    "        \"dis_from_border\": gridded_ds.dis_from_border.data,\n",
    "        \"glacier_mask\": gridded_ds.glacier_mask.data,\n",
    "        \"millan_ice_thickness\": gridded_ds.millan_ice_thickness.data,\n",
    "        \"hugonnet_dhdt\": gridded_ds.hugonnet_dhdt.data,\n",
    "        \"ID\": gdir.rgi_id,\n",
    "    }\n",
    "    print(gridded_ds.keys())\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f1fbc",
   "metadata": {},
   "source": [
    "### Set RGI version and region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43d6e51",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'dfile' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m rgi_region \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m11\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Central Europe\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m rgi_version \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m7\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m rgi_dir \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mget_rgi_dir(version\u001b[39m=\u001b[39;49mrgi_version)\n",
      "File \u001b[0;32m~/mambaforge/envs/oggm_env/lib/python3.11/site-packages/oggm/utils/_downloads.py:1885\u001b[0m, in \u001b[0;36mget_rgi_dir\u001b[0;34m(version, reset)\u001b[0m\n\u001b[1;32m   1867\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Path to the RGI directory.\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \n\u001b[1;32m   1869\u001b[0m \u001b[39mIf the RGI files are not present, download them.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1881\u001b[0m \u001b[39m    path to the RGI directory\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[39mwith\u001b[39;00m get_lock():\n\u001b[0;32m-> 1885\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_rgi_dir_unlocked(version\u001b[39m=\u001b[39;49mversion, reset\u001b[39m=\u001b[39;49mreset)\n",
      "File \u001b[0;32m~/mambaforge/envs/oggm_env/lib/python3.11/site-packages/oggm/utils/_downloads.py:1919\u001b[0m, in \u001b[0;36m_get_rgi_dir_unlocked\u001b[0;34m(version, reset)\u001b[0m\n\u001b[1;32m   1914\u001b[0m test_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(rgi_dir,\n\u001b[1;32m   1915\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39m*_rgi*\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_manifest.txt\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(version))\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(glob\u001b[39m.\u001b[39mglob(test_file)) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1918\u001b[0m     \u001b[39m# if not there download it\u001b[39;00m\n\u001b[0;32m-> 1919\u001b[0m     ofile \u001b[39m=\u001b[39m file_downloader(dfile, reset\u001b[39m=\u001b[39mreset)\n\u001b[1;32m   1920\u001b[0m     \u001b[39m# Extract root\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(ofile) \u001b[39mas\u001b[39;00m zf:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'dfile' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "rgi_region = \"11\"  # Central Europe\n",
    "rgi_version = \"7\"\n",
    "rgi_dir = utils.get_rgi_dir(version=rgi_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5450efcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'dfile' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m rgi_region \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m11\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Central Europe\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m rgi_version \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m7\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m rgi_dir \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mget_rgi_dir(version\u001b[39m=\u001b[39;49mrgi_version)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m path \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_rgi_region_file(region\u001b[39m=\u001b[39mrgi_region, version\u001b[39m=\u001b[39mrgi_version)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bachtzack01.ethz.ch/home/vmarijn/scratch/MassBalanceMachine/regions/Switzerland/OGGM-datapulling.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m rgidf \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39mread_file(path)\n",
      "File \u001b[0;32m~/mambaforge/envs/oggm_env/lib/python3.11/site-packages/oggm/utils/_downloads.py:1885\u001b[0m, in \u001b[0;36mget_rgi_dir\u001b[0;34m(version, reset)\u001b[0m\n\u001b[1;32m   1867\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Path to the RGI directory.\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \n\u001b[1;32m   1869\u001b[0m \u001b[39mIf the RGI files are not present, download them.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1881\u001b[0m \u001b[39m    path to the RGI directory\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m \u001b[39mwith\u001b[39;00m get_lock():\n\u001b[0;32m-> 1885\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_rgi_dir_unlocked(version\u001b[39m=\u001b[39;49mversion, reset\u001b[39m=\u001b[39;49mreset)\n",
      "File \u001b[0;32m~/mambaforge/envs/oggm_env/lib/python3.11/site-packages/oggm/utils/_downloads.py:1919\u001b[0m, in \u001b[0;36m_get_rgi_dir_unlocked\u001b[0;34m(version, reset)\u001b[0m\n\u001b[1;32m   1914\u001b[0m test_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(rgi_dir,\n\u001b[1;32m   1915\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39m*_rgi*\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_manifest.txt\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(version))\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(glob\u001b[39m.\u001b[39mglob(test_file)) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1918\u001b[0m     \u001b[39m# if not there download it\u001b[39;00m\n\u001b[0;32m-> 1919\u001b[0m     ofile \u001b[39m=\u001b[39m file_downloader(dfile, reset\u001b[39m=\u001b[39mreset)\n\u001b[1;32m   1920\u001b[0m     \u001b[39m# Extract root\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(ofile) \u001b[39mas\u001b[39;00m zf:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'dfile' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "\n",
    "path = utils.get_rgi_region_file(region=rgi_region, version=rgi_version)\n",
    "rgidf = gpd.read_file(path)\n",
    "\n",
    "# We use the directories with the shop data in it: \"W5E5_w_data\"\n",
    "base_url = \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\"\n",
    "gdirs = workflow.init_glacier_directories(\n",
    "    rgidf,\n",
    "    from_prepro_level=3,\n",
    "    prepro_base_url=base_url,\n",
    "    prepro_border=10,\n",
    "    reset=True,\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested tasks\n",
    "task_list = [\n",
    "    tasks.gridded_attributes,\n",
    "    tasks.gridded_mb_attributes,\n",
    "    get_gridded_features,\n",
    "]\n",
    "for task in task_list:\n",
    "    workflow.execute_entity_task(task, gdirs, print_log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f985a0",
   "metadata": {},
   "source": [
    "## Get attributes from all stakes in Switzerland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65393b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate years:\n",
    "def remove_dupl_years(df_stake):\n",
    "    all_years = []\n",
    "    rows = []\n",
    "    for row_nb in range(len(df_stake)):\n",
    "        year = df_stake.date_fix0.iloc[row_nb].year\n",
    "        if year not in all_years:\n",
    "            all_years.append(year)\n",
    "            rows.append(row_nb)\n",
    "    return df_stake.iloc[rows]\n",
    "\n",
    "\n",
    "def read_stake_csv(path_latloncoord, fileName, coi):\n",
    "    dfStake = pd.read_csv(\n",
    "        path_latloncoord + fileName,\n",
    "        sep=\",\",\n",
    "        parse_dates=[\"date_fix0\", \"date_fix1\"],\n",
    "        header=0,\n",
    "    ).drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)\n",
    "    dfStake = dfStake.drop_duplicates()\n",
    "    dfStake = remove_dupl_years(dfStake).sort_values(by=\"date_fix0\")\n",
    "\n",
    "    dfStake = dfStake[coi]\n",
    "    return dfStake\n",
    "\n",
    "def createPath(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "# empties a folder\n",
    "def emptyfolder(path):\n",
    "    if os.path.exists(path):\n",
    "        onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "        for f in onlyfiles:\n",
    "            os.remove(path + f)\n",
    "    else:\n",
    "        createPath(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stakes numbers for Swiss glaciers:\n",
    "path_GLAMOS_csv = '../../../data/MB_modeling/GLAMOS/index/csv_files/massbalance/raw_csv/'\n",
    "onlyfiles = [\n",
    "    f for f in listdir(path_GLAMOS_csv) if isfile(join(path_GLAMOS_csv, f))\n",
    "]\n",
    "\n",
    "glStakesNum = {}\n",
    "glStakes = {}\n",
    "for f in onlyfiles:\n",
    "    gl = f.split('_')[0]\n",
    "    if gl not in glStakesNum.keys():\n",
    "        glStakesNum[gl] = 1\n",
    "        glStakes[gl] = [f]\n",
    "    else:\n",
    "        glStakesNum[gl] = glStakesNum[gl] + 1\n",
    "        glStakes[gl].append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45fde0f",
   "metadata": {},
   "source": [
    "### Add OGGM data to Swiss stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14174cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables of interest from oggm\n",
    "voi = [\"aspect\", \"slope\", \"dis_from_border\", \"topo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640d060",
   "metadata": {},
   "source": [
    "#### Single stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad677b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additionnal information to all stakes:\n",
    "path_latloncoord = (\n",
    "    \"../../../data/MB_modeling/GLAMOS/index/csv_files/massbalance/WGSlatloncoord/\"\n",
    ")\n",
    "path_save = \"../../../data/MB_modeling/GLAMOS/index/csv_files/massbalance/glacierattr/\"\n",
    "# First empty folder\n",
    "emptyfolder(path_save)\n",
    "\n",
    "for i in tqdm(range(len(glStakes.keys()))):\n",
    "    key = list(glStakes.keys())[i]\n",
    "    for fileName in glStakes[key]:\n",
    "        coi = [\n",
    "            \"vaw_id\",\n",
    "            \"sgi_id\",\n",
    "            \"rgi_id\",\n",
    "            \"glims_id\",\n",
    "            \"date_fix0\",\n",
    "            \"date_fix1\",\n",
    "            \"date0\",\n",
    "            \"date1\",\n",
    "            \"date_smeas\",\n",
    "            \"lat\",\n",
    "            \"lon\",\n",
    "            \"height\",\n",
    "            \"b_a_fix\",\n",
    "            \"b_w_fix\",\n",
    "        ]\n",
    "        df_stake = read_stake_csv(path_latloncoord, fileName, coi)\n",
    "\n",
    "        # coordinates of stake:\n",
    "        lat_stake = df_stake.lat.unique()[0]\n",
    "        lon_stake = df_stake.lon.unique()[0]\n",
    "\n",
    "        # RGI ID of stake\n",
    "        rgi_id_stake = df_stake.rgi_id.iloc[0]\n",
    "\n",
    "        # get oggm data for that RGI ID\n",
    "\n",
    "        for gdir in gdirs:\n",
    "            if gdir.rgi_id == rgi_id_stake:\n",
    "                break\n",
    "        with xr.open_dataset(gdir.get_filepath(\"gridded_data\")) as ds:\n",
    "            ds = ds.load()\n",
    "\n",
    "        # transform stake coord to glacier system:\n",
    "        transf = pyproj.Transformer.from_proj(salem.wgs84,\n",
    "                                              gdir.grid.proj,\n",
    "                                              always_xy=True)\n",
    "        x_stake, y_stake = transf.transform(lon_stake, lat_stake)  # x,y stake\n",
    "\n",
    "        # Get glacier variables closest to these coordinates:\n",
    "        stake = ds.sel(x=x_stake, y=y_stake, method=\"nearest\")\n",
    "\n",
    "        # Calculate min, max and median topography of glacier:\n",
    "        min_glacier = ds.where(ds.glacier_mask == 1).topo.min().values\n",
    "        max_glacier = ds.where(ds.glacier_mask == 1).topo.max().values\n",
    "        med_glacier = ds.where(ds.glacier_mask == 1).topo.median().values\n",
    "\n",
    "        # Select variables of interest:\n",
    "        stake_var = stake[voi]\n",
    "        stake_var_df = stake_var.to_pandas()\n",
    "\n",
    "        stake_var_df['min_el_gl'] = min_glacier\n",
    "        stake_var_df['max_el_gl'] = max_glacier\n",
    "        stake_var_df['med_el_gl'] = med_glacier\n",
    "\n",
    "        for var in stake_var_df.index:\n",
    "            df_stake[var] = [\n",
    "                stake_var_df.loc[var] for i in range(len(df_stake))\n",
    "            ]\n",
    "        df_stake.to_csv(path_save + fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92ef45",
   "metadata": {},
   "source": [
    "#### Multi stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additionnal information to all stakes:\n",
    "path_latloncoord = (\n",
    "    \"../../../data/MB_modeling_multi/GLAMOS/index_time/csv_files/WGSlatloncoord/\"\n",
    ")\n",
    "path_save = \"../../../data/MB_modeling_multi/GLAMOS/index_time/csv_files/glacierattr/\"\n",
    "# First empty folder\n",
    "emptyfolder(path_save)\n",
    "\n",
    "for i in tqdm(range(len(glStakes.keys()))):\n",
    "    key = list(glStakes.keys())[i]\n",
    "    for fileName in glStakes[key]:\n",
    "        coi = [\n",
    "            \"vaw_id\",\n",
    "            \"sgi_id\",\n",
    "            \"rgi_id\",\n",
    "            \"glims_id\",\n",
    "            \"date_fix0\",\n",
    "            \"date_fix1\",\n",
    "            \"date0\",\n",
    "            \"date1\",\n",
    "            \"date_smeas\",\n",
    "            \"lat\",\n",
    "            \"lon\",\n",
    "            \"height\",\n",
    "            \"b_a_fix\",\n",
    "            \"b_w_fix\",\n",
    "        ]\n",
    "        df_stake = read_stake_csv(path_latloncoord, fileName, coi)\n",
    "\n",
    "        # coordinates of stake:\n",
    "        lat_stake = df_stake.lat.unique()[0]\n",
    "        lon_stake = df_stake.lon.unique()[0]\n",
    "        \n",
    "        # RGI ID of stake\n",
    "        rgi_id_stake = df_stake.rgi_id.iloc[0]\n",
    "        \n",
    "        # get oggm data for that RGI ID\n",
    "\n",
    "        for gdir in gdirs:\n",
    "            if gdir.rgi_id == rgi_id_stake:\n",
    "                break\n",
    "        with xr.open_dataset(gdir.get_filepath(\"gridded_data\")) as ds:\n",
    "            ds = ds.load()\n",
    "\n",
    "        # transform stake coord to glacier system:\n",
    "        transf = pyproj.Transformer.from_proj(\n",
    "            salem.wgs84, gdir.grid.proj, always_xy=True\n",
    "        )\n",
    "        x_stake, y_stake = transf.transform(lon_stake, lat_stake) # x,y stake\n",
    "\n",
    "        # Get glacier variables closest to these coordinates:\n",
    "        stake = ds.sel(x=x_stake, y=y_stake, method=\"nearest\")\n",
    "        \n",
    "        # Calculate min, max and median topography of glacier:\n",
    "        min_glacier = ds.where(ds.glacier_mask == 1).topo.min().values\n",
    "        max_glacier = ds.where(ds.glacier_mask == 1).topo.max().values\n",
    "        med_glacier = ds.where(ds.glacier_mask == 1).topo.median().values\n",
    "\n",
    "        # Select variables of interest:\n",
    "        stake_var = stake[voi]\n",
    "        stake_var_df = stake_var.to_pandas()\n",
    "        \n",
    "        stake_var_df['min_el_gl'] = min_glacier\n",
    "        stake_var_df['max_el_gl'] = max_glacier\n",
    "        stake_var_df['med_el_gl'] = med_glacier\n",
    "\n",
    "        for var in stake_var_df.index:\n",
    "            df_stake[var] = [stake_var_df.loc[var] for i in range(len(df_stake))]\n",
    "        df_stake.to_csv(path_save + fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stake_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.where(ds.glacier_mask == 1).topo.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'pers_P24_mb.csv'\n",
    "df_stake = read_stake_csv(path_latloncoord, fileName, coi)\n",
    "rgi_id_stake = df_stake.rgi_id.iloc[0]\n",
    "print(rgi_id_stake)\n",
    "\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == rgi_id_stake:\n",
    "        break\n",
    "with xr.open_dataset(gdir.get_filepath(\"gridded_data\")) as ds:\n",
    "    ds = ds.load()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efaf95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b2155",
   "metadata": {},
   "source": [
    "#### Example of Aletsch :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49360abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Aletsch\n",
    "# Get coordinates and time of file for this stake:\n",
    "fileName = \"aletsch_P0_mb.csv\"\n",
    "coi = [\n",
    "    \"glims_id\",\n",
    "    \"sgi_id\",\n",
    "    \"rgi_id\",\n",
    "    \"glims_id\",\n",
    "    \"date_fix0\",\n",
    "    \"date_fix1\",\n",
    "    \"date0\",\n",
    "    \"date1\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"height\",\n",
    "    \"b_a_fix\",\n",
    "    \"b_w_fix\"\n",
    "]\n",
    "df_stake = read_stake_csv(path_latloncoord, fileName, coi)\n",
    "\n",
    "print(lat_stake, lon_stake, rgi_id_stake)\n",
    "\n",
    "for gdir in gdirs:\n",
    "    # print(gdir.name)\n",
    "    if gdir.rgi_id == rgi_id_stake:\n",
    "        break\n",
    "with xr.open_dataset(gdir.get_filepath(\"gridded_data\")) as ds:\n",
    "    ds = ds.load()\n",
    "\n",
    "# transform stake coord to glacier system:\n",
    "transf = pyproj.Transformer.from_proj(salem.wgs84, gdir.grid.proj, always_xy=True)\n",
    "x_stake, y_stake = transf.transform(lon_stake, lat_stake)\n",
    "\n",
    "# Get glacier variables at these coordinates:\n",
    "stake = ds.sel(x=x_stake, y=y_stake, method=\"nearest\")\n",
    "\n",
    "# variables of interest:\n",
    "voi = [\"aspect\", \"slope\", \"dis_from_border\", \"topo\"]\n",
    "stake_var = stake[voi]\n",
    "stake_var_df = stake_var.to_pandas()\n",
    "stake_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad6465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fecc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "277.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

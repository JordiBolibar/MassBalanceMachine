{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbe8423",
   "metadata": {},
   "source": [
    "**@Author:** Julian Biesheuvel  \n",
    "**Email:** j.p.biesheuvel@student.tudelft.nl  \n",
    "**Date Created:** 17/06/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff601bcba114b15",
   "metadata": {},
   "source": [
    "# XGBoost Model Training with Custom Loss Function - for Regaional Stake Data\n",
    "\n",
    "In this notebook, a XGBoost model, with a custom loss function, is trained to predict the surface mass balance for glaciers in the designated  region with a **monthly resolution**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T23:48:15.091045Z",
     "start_time": "2024-06-18T23:48:09.876711Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import os.path\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from re import match\n",
    "from pathlib import Path\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sys.path.append('.././scripts-model-training')\n",
    "\n",
    "from plotting_methods import *\n",
    "from preparation_data import *\n",
    "from model_methods import *\n",
    "from custom_xgboost_regressor import CustomXGBoostRegressor\n",
    "\n",
    "xgb.set_config(verbosity=0)\n",
    "\n",
    "PLOTTING = False\n",
    "RANDOM_SEED = 42\n",
    "FILE_DIR = '.././data/files/'\n",
    "FILE_NAME = 'region_stake_data_cleaned.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fceaa8c16b259",
   "metadata": {},
   "source": [
    "## 1. Import the Dataset and Prepare the Data for Training, Testing, and Cross Validation\n",
    "\n",
    "Remove the records with missing annual or seasonal surface mass balances, and randomly split the dataset for the annual and seasonal surface mass balances into a training and test dataset.  \n",
    "\n",
    "**Note**: geographical locations can introduce variations in climate variables that may affect model training and evaluation. Whether to split based on geography depends on whether these variations are significant enough to impact the model's performance and generalization capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811850d6b4d3861",
   "metadata": {},
   "source": [
    "### 1.1 Prepare Variables of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7da552cd99ce40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T23:48:15.365669Z",
     "start_time": "2024-06-18T23:48:15.091045Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset that is cleaned (None RGIIds are removed)\n",
    "path_name = os.path.join(FILE_DIR, FILE_NAME)\n",
    "df = pd.read_csv(filepath_or_buffer=path_name)\n",
    "\n",
    "# Define a dictionary for the seasonal data\n",
    "seasons = {\n",
    "    'annual': {'column': 'ba_stratigraphic', 'n_months': 12},\n",
    "    'winter': {'column': 'bw_stratigraphic', 'n_months': 7},\n",
    "    'summer': {'column': 'bs_stratigraphic', 'n_months': 5}\n",
    "}\n",
    "\n",
    "# Define which columns are of interest (vois: variables of interest), please see the metadata file for the ERA5-Land data with all the variable names\n",
    "# Here, the variables of interest can be adjusted to suit specific needs or preferences.\n",
    "vois_climate = ['t2m', 'tp', 'sshf', 'slhf', 'ssrd', 'fal', 'str']\n",
    "\n",
    "# Create a dictionary of all the columns in the dataset that match the variables of interest of the ERA5-Land data\n",
    "vois_climate_columns = {voi: [col for col in df.columns.values if re.match(f'{voi}_[a-zA-Z]*', col)] for voi in vois_climate}\n",
    "\n",
    "# Specify the column names for the seasonal and annual mass balance columns in the dataset, as well as the column names for the topographical features obtained from OGGM,\n",
    "# and the column name that holds the hydrological years.\n",
    "# Change the variables names to the ones that match in the dataframe.\n",
    "smb_types = ['ba_stratigraphic', 'bw_stratigraphic', 'bs_stratigraphic']\n",
    "vois_topo_columns = ['aspect', 'elevation', 'height_diff', 'slope'] \n",
    "misc_columns = ['yr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ba7510671f973",
   "metadata": {},
   "source": [
    "### 1.2 Prepare Train, Validation, and Test Data\n",
    "\n",
    "Define the ```num_samples``` as the desired number of records to increase the temporal resolution from a season scale to a monthly scale. Please be aware that the number of records will increase significantly, which will, in turn, lengthen the model's training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2db71f70dea76d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T23:48:18.999369Z",
     "start_time": "2024-06-18T23:48:15.365669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of entries in train/test for annual surface mass balances: 828/360, train: annual_train, and test: annual_test\n",
      "Amount of entries in train/test for winter surface mass balances: 490/210, train: winter_train, and test: winter_test\n",
      "Amount of entries in train/test for summer surface mass balances: 345/150, train: summer_train, and test: summer_test\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of datasets, containing training and testing datasets for an annual, winter, and summer period. \n",
    "NUM_SAMPLES = 100\n",
    "datasets = create_model_data(df, seasons, vois_climate_columns, vois_topo_columns, smb_types, misc_columns, RANDOM_SEED, num_samples=NUM_SAMPLES)\n",
    "\n",
    "# In this case the focus is to handle data from all periods, and therefore the dataset 'all' is selected. For summer and winter, select 'summer' and 'winter' respectively. \n",
    "df_X_train, df_y_train, X_train, y_train, splits_train = make_train_test_split(datasets['all']['train'], 5)\n",
    "df_X_test, df_y_test, X_test, y_test, splits_test = make_train_test_split(datasets['all']['test'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ed6d62cee38b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T23:48:19.856636Z",
     "start_time": "2024-06-18T23:48:19.000933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot for each fold of the cross validation a histogram of the number of samples per period (annual, summer, winter) in the training and validation dataset. \n",
    "plot_fold_distribution(splits_train, df_X_train, PLOTTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0370881e42fa30",
   "metadata": {},
   "source": [
    "## 2. Define the Custom Loss Function and XGBoost Regressor Function\n",
    "\n",
    "Please see the following files for the custom loss function and the custom estimator class bases on XGBRegressor:\n",
    "- ```model_methods.py``` -> ```custom_mse_metadata()```, will be used with the custom XGBRegressor class\n",
    "- ```CustomXGBoostRegressor.py``` -> ```CustomXGBoostRegressor()```\n",
    "    - TODO: modify ```def init()``` to retrieve keyword arguments for number of columns of metadata, with default = None, to allow for flexibility in the number of metadata columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb496950896a22a",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76667da6ef7ffed7",
   "metadata": {},
   "source": [
    "### 3.1 Train XGBoost Model with Custom MSE Scorer and GridSearchCV (on a subset of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c6020a0c267a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T23:48:42.601684Z",
     "start_time": "2024-06-18T23:48:19.856636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "# NOTE: Based on the defined parameter grid values for each learning parameter, user warnings may occur if one or more of the training scores are non-finite. This issue has not yet been resolved.\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],#, 5, 6, 7, 8],\n",
    "    'learning_rate': [0.01, 0.1],#, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma':[0]#, 1, 10],\n",
    "    }\n",
    "\n",
    "# Create a new XGBoostRegressor object that uses the custom loss function\n",
    "xgb_model = CustomXGBoostRegressor(metadata_shape=3)\n",
    "\n",
    "# IMPORTANT: n_jobs=-1 will use all the available cores. Please change to the desired number of cores.\n",
    "clf = GridSearchCV(xgb_model, \n",
    "                   param_grid, \n",
    "                   cv=splits_train,\n",
    "                   verbose=0, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True, \n",
    "                   return_train_score=True) \n",
    "\n",
    "# Perform the cross validation for the provided parameter grid\n",
    "clf.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "# Set the best model to the estimator that performed best in the gridsearch \n",
    "best_model = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59e9ee99da81bd",
   "metadata": {},
   "source": [
    "### 3.2 Plot the Score per Learning Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e882e4c4fd72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T23:48:43.385848Z",
     "start_time": "2024-06-18T23:48:42.601684Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_gsearch_results(clf, '',PLOTTING) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0a5123c2f1cab",
   "metadata": {},
   "source": [
    "### 3.3 Plot the Results of the Grid Search per Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee4a903a926efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T23:48:49.523461Z",
     "start_time": "2024-06-18T23:48:43.385848Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_prediction_validation(X_train, y_train, best_model, splits_train, '', PLOTTING, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f548c4cc204775",
   "metadata": {},
   "source": [
    "### 3.3 Plot the Annual and Seasonal Aggregates of the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc1475ae846e6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T23:49:05.325044Z",
     "start_time": "2024-06-18T23:48:49.523461Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_prediction_per_season(PLOTTING, 'Train', X_train, y_train, splits_train, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b34a6aec6d81a",
   "metadata": {},
   "source": [
    "## 4. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e263ef5648cbbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T23:49:12.448719Z",
     "start_time": "2024-06-18T23:49:05.325044Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_prediction_per_season(PLOTTING, 'Test', X_test, y_test, splits_test, best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
